version: '3.1'
services:
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports: 
      - "8080:80"
    networks:
      - main
  
  nginx:
    image: nginx:alpine
    container_name: nginx
    ports: 
      - "80:80"
    depends_on:
      - frontend
    networks:
      - main

  backend:
    build: ./backend
    container_name: backend
    volumes:
      - ./backend:/backend
    command: ["python", "-m", "backend"]
    ports: 
      - "5000:5000"
    networks:
      - main

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - 11434:11434
    volumes:
      - ./ollama_data:/root/.ollama
    command: serve && run mistral
    healthcheck:
      test: ollama list || exit 1
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 3000M
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - main

  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: whisper
    ports:
      - 9000:9000
    environment:
      - ASR_MODEL=medium
      - ASR_ENGINE=faster_whisper
    deploy:
      resources:
        limits:
          cpus: '0.5'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    networks:
      - main

  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    command: celery -A backend.celery.worker.celery worker
    depends_on:
      - redis
    networks:
      - main

  redis:
    image: redis:7
    ports:
      - 6379:6379
    networks:
      - main

  tg_bot:
    build:
      context: ./tg_bot
      dockerfile: Dockerfile
    networks:
      - main

networks:
  main: